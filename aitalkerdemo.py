import json
import os
import random
import gradio as gr
import time
from datetime import datetime
from zhconv import convert
from LLM import LLM
from ASR import WhisperASR
from TFG import SadTalker
from TTS import EdgeTTS
from src.cost_time import calculate_time
import PyPDF2
from docx import Document
# ç¯å¢ƒé…ç½®
os.environ["GRADIO_TEMP_DIR"] = './temp'

# ç³»ç»Ÿæè¿°
description = """<p style="text-align: center; font-weight: bold;">
    <span style="font-size: 28px;">Mentor Nekomiya: è½¯ä»¶æŠ€æœ¯å²— AI é¢è¯•å®˜ </span><br> 
    <span>æ”¯æŒæœ¬åœ°ç®€å†ä¸Šä¼ å’Œæ¨¡æ‹Ÿé¢è¯•ï¼Œç”Ÿæˆé€¼çœŸé¢è¯•åœºæ™¯</span>
</p>
"""

# ç³»ç»Ÿå‚æ•°é…ç½®
blink_every = True
size_of_image = 256
preprocess_type = 'crop'
facerender = 'facevid2vid'
enhancer = False
is_still_mode = False
exp_weight = 1

# æ•°å­—äººå½¢è±¡é…ç½®
pic_path = "girl.png"
crop_pic_path = "./inputs/first_frame_dir_girl/girl.png"
first_coeff_path = "./inputs/first_frame_dir_girl/girl.mat"
crop_info = (
    (403, 403), (19, 30, 502, 513), [40.05956541381802, 40.17324339233366, 443.7892505041507, 443.9029284826663])

use_ref_video = False
ref_video = None
ref_info = 'pose'
use_idle_mode = False
length_of_audio = 5

# # æœ¬åœ°ç®€å†å­˜å‚¨è·¯å¾„
UPLOAD_FOLDER = './uploads'
os.makedirs(UPLOAD_FOLDER, exist_ok=True)
# ç®€å†é…ç½®
CV_PATH = "./data/candidate_cv.docx"  # å›ºå®šç®€å†è·¯å¾„

def parse_cv(file_path):
    """è§£ææœ¬åœ°ç®€å†æ–‡ä»¶"""
    try:
        ext = os.path.splitext(file_path)[1].lower()
        
        if ext == '.pdf':
            with open(file_path, 'rb') as f:
                reader = PyPDF2.PdfReader(f)
                return '\n'.join([page.extract_text() for page in reader.pages])
        
        elif ext == '.docx':
            doc = Document(file_path)
            print(doc)
            return '\n'.join([para.text for para in doc.paragraphs if para.text])[:2000]
        
        elif ext == '.txt':
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
        
        else:
            raise ValueError("ä¸æ”¯æŒçš„ç®€å†æ ¼å¼")
    except Exception as e:
        raise RuntimeError(f"ç®€å†è§£æå¤±è´¥: {str(e)}")

@calculate_time
def Asr(audio):
    """è¯­éŸ³è¯†åˆ«å¤„ç†"""
    try:
        question = asr.transcribe(audio)
        return convert(question, 'zh-cn')
    except Exception as e:
        print("ASR Error:", e)
        return "è¯­éŸ³è¯†åˆ«å¤±è´¥ï¼Œè¯·é‡è¯•"

@calculate_time
def LLM_response(question, voice='zh-CN-YunyangNeural', rate=0, volume=100, pitch=0, prompt=None):
    """ç”Ÿæˆå›ç­”å¹¶åˆæˆè¯­éŸ³"""
    print(question)
    print(prompt)
    answer = llm.generate(question, prompt)
    print("é¢è¯•å®˜å›ç­”:", answer)
    
    # EdgeTTSç”Ÿæˆè¯­éŸ³
    output_path = os.path.join(UPLOAD_FOLDER, f"answer_{int(time.time())}.wav")
    tts.predict(answer, voice, rate, volume, pitch, output_path)
    
    return output_path, answer

@calculate_time
def Talker_response(chatbot, source_image, text, 
                   prompt=None, voice='zh-CN-YunyangNeural', 
                   rate=0, volume=100, pitch=0, batch_size=2):
    """ç”Ÿæˆæ•°å­—äººè§†é¢‘å“åº”"""
    prompt = 'ä½ æ˜¯ä¸€ä¸ªé¢è¯•å®˜è¯·é—®å‡ºç¬¬ä¸€ä¸ªé—®é¢˜'
    driven_audio, response = LLM_response(text, voice, rate, volume, pitch, prompt)
    
    # ç”Ÿæˆéšæœºå¤´éƒ¨åŠ¨ä½œ
    pose_style = random.randint(0, 45)
    
    video = talker.test(pic_path,
                        crop_pic_path,
                        first_coeff_path,
                        crop_info,
                        source_image,
                        driven_audio,
                        )
    return video, chatbot + [(text if prompt == None else 'å¼€å§‹é¢è¯•', response)]

# def save_uploaded_cv(file):
#     """ä¿å­˜ä¸Šä¼ çš„ç®€å†åˆ°æœ¬åœ°"""
#     timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
#     save_path = os.path.join(UPLOAD_FOLDER, f"cv_{timestamp}{os.path.splitext(file.name)[-1]}")
#     with open(save_path, 'wb') as f:
#         f.write(file.read())
#     return save_path

# def read_latest_cv(chatbot, source_image, voice, rate, volume, pitch, batch_size):
#     """è¯»å–æœ€æ–°ä¸Šä¼ çš„ç®€å†"""
#     try:
#         cv_files = [f for f in os.listdir(UPLOAD_FOLDER) if f.startswith('cv_')]
#         if not cv_files:
#             return None, chatbot + [("ç³»ç»Ÿ", "æœªæ‰¾åˆ°ç®€å†æ–‡ä»¶ï¼Œè¯·å…ˆä¸Šä¼ ")]
            
#         latest_cv = max(cv_files, key=lambda x: os.path.getctime(os.path.join(UPLOAD_FOLDER, x)))
#         with open(os.path.join(UPLOAD_FOLDER, latest_cv), 'r', encoding='utf-8') as f:
#             cv_content = f.read()
        
#         return Talker_response(
#             chatbot, source_image, 
#             f"å€™é€‰äººç®€å†å†…å®¹ï¼š{cv_content[:1000]}...ï¼ˆå·²æˆªæ–­ï¼‰",
#             'è¯·æ ¹æ®ç®€å†å†…å®¹æå‡ºç¬¬ä¸€ä¸ªé¢è¯•é—®é¢˜',
#             voice, rate, volume, pitch, batch_size
#         )
#     except Exception as e:
#         print("è¯»å–ç®€å†å¤±è´¥:", e)
#         return None, chatbot + [("ç³»ç»Ÿ", "ç®€å†è¯»å–å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼")]

# def start_interview():
#     """å¯åŠ¨é¢è¯•æµç¨‹"""
#     try:
#         # è¯»å–å¹¶è§£æç®€å†
#         cv_content = parse_cv(CV_PATH)
#         prompt = f"æ ¹æ®ä»¥ä¸‹ç®€å†å†…å®¹å¼€å§‹æŠ€æœ¯é¢è¯•ï¼š\n{cv_content[:1000]}..."
        
#         # ç”Ÿæˆé¦–ä¸ªé—®é¢˜
#         initial_question = llm.generate(prompt)
#         source_image = gr.Image(label="é¢è¯•å®˜å½¢è±¡", type="filepath", elem_id="img2img_image",
#                                                         value='examples/source_image/full4.jpeg',
#                                                         width=512)
#         return Talker_response([], source_image,initial_question)
#     except Exception as e:
#         print("é¢è¯•å¯åŠ¨å¤±è´¥:", e)
#         return None, [("ç³»ç»Ÿ", f"é¢è¯•åˆå§‹åŒ–å¤±è´¥: {str(e)}")]
def clear_session():
    """æ¸…ç©ºä¼šè¯"""
    llm.clear_history()
    return '', []

def main():
    """é¢„å¤„ç†é»˜è®¤çš„ç®€å†"""
    
    """ä¸»ç•Œé¢"""
    with gr.Blocks(analytics_enabled=False, title='AIé¢è¯•å®˜') as inference:
        gr.HTML(description)
        
        with gr.Row(equal_height=False):
            # å·¦ä¾§æ§åˆ¶é¢æ¿
            with gr.Column(variant='panel'):
                chatbot = gr.Chatbot(label="å¯¹è¯è®°å½•", height=400)
                question_audio = gr.Audio(sources=['microphone'], 
                                         type="filepath", 
                                         label='è¯­éŸ³è¾“å…¥')
                input_text = gr.Textbox(label="æ–‡å­—è¾“å…¥", lines=3)
                #input_text = "ä½ å¥½é¢è¯•å®˜"
                with gr.Accordion("Advanced Settings(é«˜çº§è®¾ç½®) ",
                                              open=False):
                                source_image = gr.Image(label="é¢è¯•å®˜å½¢è±¡", type="filepath", elem_id="img2img_image",
                                                        value='examples/source_image/full4.jpeg',
                                                        width=512)
                                # voice = gr.Dropdown([],
                                #                     value='zh-CN-XiaoxiaoNeural',
                                #                     label="Voice")
                                # rate = gr.Slider(minimum=-100,
                                #                  maximum=100,
                                #                  value=0,
                                #                  step=1.0,
                                #                  label='Rate')
                                # volume = gr.Slider(minimum=0,
                                #                    maximum=100,
                                #                    value=100,
                                #                    step=1,
                                #                    label='Volume')
                                # pitch = gr.Slider(minimum=-100,
                                #                   maximum=100,
                                #                   value=0,
                                #                   step=1,
                                #                   label='Pitch')
                                # batch_size = gr.Slider(minimum=1,
                                #                        maximum=10,
                                #                        value=2,
                                #                        step=1,
                                #                        label='Talker Batch size')

                
                with gr.Row():
                    asr_btn = gr.Button("ğŸ¤ è¯­éŸ³è½¬æ–‡å­—")
                    submit_btn = gr.Button("ğŸš€ å‘é€", variant='primary')
                    clear_btn = gr.Button("ğŸ§¹ æ¸…ç©ºè®°å½•")

            # å³ä¾§æ˜¾ç¤ºé¢æ¿
            with gr.Column(variant='panel'):
                gen_video = gr.Video(label="é¢è¯•å®˜è§†é¢‘", format="mp4", autoplay=True)

        # äº‹ä»¶ç»‘å®š
        submit_btn.click(fn=Talker_response,
                         inputs=[chatbot,
                                 source_image,
                                 input_text],
                         outputs=[gen_video, chatbot])
        asr_btn.click(Asr, inputs=question_audio, outputs=input_text)
        clear_btn.click(lambda: ([], ""), outputs=[chatbot, input_text])
        
        #è‡ªåŠ¨å¯åŠ¨é¢è¯•
        # inference.load(
        #     fn=start_interview,
        #     outputs=[gen_video, chatbot]
        # )
#         cv_content = parse_cv(CV_PATH)
#         print(cv_content)
#         # gen_video, chatbot = Talker_response(chatbot, source_image, cv_content + 'é¢è¯•æ­£å¼å¼€å§‹ï¼Œä»¥ä¸‹æ˜¯è¿™ä½åŒå­¦çš„ç®€å†ã€‚è¯·ç»™å‡ºç¬¬ä¸€ä¸ªé¢˜ç›®ã€‚')
#         gen_video, chatbot = Talker_response(chatbot, source_image, 'é¢è¯•æ­£å¼å¼€å§‹ï¼Œä»¥ä¸‹æ˜¯è¿™ä½åŒå­¦çš„ç®€å†ã€‚è¯·ç»™å‡ºç¬¬ä¸€ä¸ªé¢˜ç›®ã€‚')
        

    return inference

if __name__ == "__main__":
    # åˆå§‹åŒ–æ¨¡å—
    # llm_class = LLM(mode='offline')
    # llm = llm_class.init_model('Qwen', 'Qwen/Qwen-1_8B-Chat', prefix_prompt="ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„é¢è¯•å®˜")
    llm_class = LLM(mode='offline')
    llm = llm_class.init_model('Qwen', 'Qwen/Qwen-1_8B-Chat', prefix_prompt="ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„é¢è¯•å®˜")
    #llm = llm_class.init_model('ç›´æ¥å›å¤ Direct Reply')
   
    talker = SadTalker(lazy_load=True)
    asr = WhisperASR('base')
    tts = EdgeTTS()
    
    # å¯åŠ¨åº”ç”¨
    gr.close_all()
    demo = main()
    demo.queue()
    demo.launch(
        server_name="0.0.0.0",
        server_port=6006,
        debug=True
    )